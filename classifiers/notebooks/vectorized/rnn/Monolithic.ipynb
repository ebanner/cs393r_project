{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolithic Recurrent Neural Network\n",
    "\n",
    "Vectorized 2-layer fully-connected recurrent neural network with cross-entropy loss. Includes support for:\n",
    "\n",
    "- Minibatching\n",
    "- Optional gradient checking\n",
    "- L2 Regularization\n",
    "- Logging so you can see what the scores, probabilities, gradients, etc. are after every minibatch\n",
    "\n",
    "This is a shallow network with hard-coded sigmoid units. Additionally this architecture assumes each sequence is the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple('State', ['loss', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs'])\n",
    "Snapshot = namedtuple('State', ['xs', 'ys', 'Whh', 'bhh', 'Wxh', 'bxh', 'Ws', 'bs', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs', 'dhiddens', 'dhiddens_local', 'dhiddens_downstream', 'scores', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.shallow.helper import sigmoid, sigmoid_grad\n",
    "from softmax import softmax_vectorized\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    If you want to inspect the scores after each training example, the pass inspect. If you do\n",
    "    this then you better set a batch_size to 1. Otherwise you'll only ever get the scores of\n",
    "    the last training example in the minibatch\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X, ys_train, H, C,\n",
    "                 Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None,\n",
    "                 rollout=None, learning_rate=0.001, regularizer=1.,\n",
    "                gradient_checking=False, inspect=False):\n",
    "        \"\"\"Initializes recurrent neural network classifier\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : N x M 2d array containing all elements in the sequence\n",
    "        ys_train : length M list of labels\n",
    "        H : size of hidden layer\n",
    "        C : number of target classes\n",
    "        Whh : H x H 2d matrix mapping previous hidden layer to new hidden layer\n",
    "        bhh : H x 1 array of bias terms applied after Whh multiplication\n",
    "        Wxh : H x N 2d matrix mapping input at time t to hidden size\n",
    "        bxh : H x 1 array of bias terms applied after Wxh multiplication\n",
    "        Ws : C x H matrix of softmax weights\n",
    "        bs : C x 1 array of softmax biases\n",
    "        rollout : the number of tokens to train the rnn on in one go\n",
    "        learning_rate : learning rate constant\n",
    "        regularizer : regularization constant\n",
    "        gradient_checking : boolean whether to perform gradient checking during training\n",
    "        inspect : boolean whether to log all data after every learning session from a training example\n",
    "        \n",
    "        \"\"\"\n",
    "        (self.N, self.M) = X.shape\n",
    "        self.H = H\n",
    "        \n",
    "        self.X_train, self.ys_train = X, ys_train\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = np.random.randn(H, H) if not type(Whh) == np.ndarray else Whh\n",
    "        self.bhh = np.random.randn(H, 1) if not type(bhh) == np.ndarray else bhh\n",
    "        self.Wxh = np.random.randn(H, self.N) if not type(Wxh) == np.ndarray else Wxh\n",
    "        self.bxh = np.random.randn(H, 1) if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = np.random.randn(C, H) if not type(Ws) == np.ndarray else Ws\n",
    "        self.bs = np.random.randn(C, 1) if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        self.rollout = self.M if not rollout else rollout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularizer = regularizer\n",
    "        \n",
    "        self.train_index = 0\n",
    "        \n",
    "        self.gradient_checking = gradient_checking\n",
    "        self.inspect = inspect\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the probability of x belonging to either class\"\"\"\n",
    "        \n",
    "        scores = self.forward_backward_prop(X, predict=True)\n",
    "        proper_scores = np.hstack([score for t, score in sorted(scores.items())])\n",
    "        \n",
    "        return proper_scores, proper_scores.argmax(axis=0)\n",
    "        \n",
    "    def forward_backward_prop(self, X=None, Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None, predict=False):\n",
    "        \"\"\"Perform forward and backward prop over a single training example\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        # Hidden and input weights\n",
    "        Whh = self.Whh if not type(Whh) == np.ndarray else Whh\n",
    "        bhh = self.bhh if not type(bhh) == np.ndarray else bhh\n",
    "        Wxh = self.Wxh if not type(Wxh) == np.ndarray else Wxh\n",
    "        bxh = self.bxh if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        Ws = self.Ws if not type(Ws) == np.ndarray else Ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        # Passed in X to predict?\n",
    "        if type(X) == np.ndarray:\n",
    "            N, T = X.shape\n",
    "            rollout = T\n",
    "            ys = np.ones(T)\n",
    "        else:\n",
    "            X = self.X_train[:, self.train_index:self.train_index+self.rollout]\n",
    "            ys = self.ys_train[self.train_index:self.train_index+self.rollout]\n",
    "            rollout = self.rollout\n",
    "        \n",
    "        # Append column of zeros to make X and Y aligned in time\n",
    "        X, ys = np.hstack([np.zeros((self.N, 1)), X]), np.hstack([np.zeros(1, dtype=np.int), ys])\n",
    "        \n",
    "        # Forward pass!\n",
    "        dWhh, dbhh = np.zeros_like(Whh), np.zeros_like(bhh)\n",
    "        dWxh, dbxh = np.zeros_like(Wxh), np.zeros_like(bxh)\n",
    "        dWs, dbs = np.zeros_like(Ws), np.zeros_like(bs)\n",
    "        \n",
    "        loss = 0.\n",
    "        hiddens, dhiddens = {t:np.ones((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        dhiddens_downstream, dhiddens_local = {t:np.zeros((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        scores, probs = {t:None for t in range(1, rollout+1)}, {t:None for t in range(1, rollout+1)}\n",
    "        for t in range(1, rollout+1):\n",
    "            # Previous hidden layer and input at time t\n",
    "            Z = (Whh @ hiddens[t-1] + bhh) + (Wxh @ X[:,[t]] + bxh)\n",
    "            hiddens[t] = sigmoid(Z)\n",
    "            \n",
    "            # Softmax\n",
    "            scores[t] = Ws @ hiddens[t] + bs\n",
    "            probs[t] = softmax_vectorized(scores[t])\n",
    "            y_hat = probs[t][ys[t]]\n",
    "\n",
    "            # Loss\n",
    "            loss += -np.log(y_hat).sum()\n",
    "\n",
    "        # Add regularization\n",
    "        loss += self.regularizer * 0.5*(np.sum(Whh**2) + np.sum(bhh**2) +\n",
    "                                        np.sum(Wxh**2) + np.sum(bxh**2) +\n",
    "                                        np.sum(Ws**2) + np.sum(bs**2))\n",
    "        if predict:\n",
    "            return scores\n",
    "        \n",
    "        # Backpropagate!\n",
    "        backwards = list(reversed(range(rollout+1)))\n",
    "        for t in backwards[:-1]:\n",
    "            # Scores\n",
    "            dscores = probs[t]\n",
    "            dscores[ys[t], 0] -= 1\n",
    "\n",
    "            # Softmax weights\n",
    "            dbs += dscores\n",
    "            dWs += dscores @ hiddens[t].T\n",
    "\n",
    "            # Karpathy optimization\n",
    "            dhiddens_local[t] = Ws.T @ dscores\n",
    "            dhiddens[t] = dhiddens_local[t] + dhiddens_downstream[t]\n",
    "            \n",
    "            dZ = sigmoid_grad(hiddens[t]) * dhiddens[t]\n",
    "\n",
    "            # Input and hidden weights\n",
    "            dbxh += dZ\n",
    "            dWxh += dZ @ X[:,[t]].T\n",
    "            dbhh += dZ\n",
    "            dWhh += dZ @ hiddens[t-1].T\n",
    "            \n",
    "            # Set up incoming hidden weight gradient for previous time step\n",
    "            dhiddens_downstream[t-1] = Whh.T @ dZ\n",
    "        \n",
    "        # Regularization\n",
    "        #\n",
    "        # Hidden and input weights\n",
    "        dWhh += (self.regularizer*Whh)\n",
    "        dbhh += (self.regularizer*bhh)\n",
    "        dWxh += (self.regularizer*Wxh)\n",
    "        dbxh += (self.regularizer*bxh)\n",
    "        \n",
    "        # Softmax weights\n",
    "        dWs += (self.regularizer*Ws)\n",
    "        dbs += (self.regularizer*bs)\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            self.xs, self.ys = str(X[:, 1:]), str(ys[1:])\n",
    "            self.scores, self.probs = scores, probs\n",
    "            self.loss = loss\n",
    "            self.dWhh, self.dbhh, self.dWxh, self.dbxh = dWhh, dbhh, dWxh, dbxh\n",
    "            self.dWs, self.dbs = dWs, dbs\n",
    "            self.dhiddens = dhiddens\n",
    "            self.dhiddens_local, self.dhiddens_downstream = dhiddens_local, dhiddens_downstream\n",
    "        \n",
    "        return State(loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Learn from a minibatch of training examples\n",
    "        \n",
    "        Run gradient descent on these examples\n",
    "        \n",
    "        \"\"\"\n",
    "        loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs = self.forward_backward_prop()\n",
    "\n",
    "        self.gradient_check(dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = self.Whh - self.learning_rate*dWhh\n",
    "        self.bhh = self.bhh - self.learning_rate*dbhh\n",
    "        self.Wxh = self.Wxh - self.learning_rate*dWxh\n",
    "        self.bxh = self.bxh - self.learning_rate*dbxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = self.Ws - self.learning_rate*dWs\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.train_index = (self.train_index+self.rollout) % self.M\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            pass\n",
    "    \n",
    "    def gradient_check(self, analytic_dWhh, analytic_dbhh, analytic_dWxh, analytic_dbxh, analytic_dWs, analytic_dbs):\n",
    "        \"\"\"Verify gradient correctness\n",
    "        \n",
    "        The analytic dWhh, dbhh, dWxh, dbxh, dWs, and dbs come from doing forward-backward\n",
    "        prop just a second ago. We numerically estimate these gradients on\n",
    "        the *same* minibatch the analytic gradients were computed from and\n",
    "        compare them to see if they are close.\n",
    "        \n",
    "        Note the same rollout is being used because this function gets\n",
    "        called *before* the update to batch_index\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.gradient_checking:\n",
    "            return\n",
    "        \n",
    "        num_dWhh, num_dbhh, num_dWxh, num_dbxh, num_dWs, num_dbs = self.numerical_gradients()\n",
    "\n",
    "        # Compute relative error\n",
    "        #\n",
    "        # Hidden and input differences\n",
    "        dWhh_error = abs(num_dWhh- analytic_dWhh) / (abs(num_dWhh) + abs(analytic_dWhh))\n",
    "        dbhh_error = abs(num_dbhh - analytic_dbhh) / (abs(num_dbhh) + abs(analytic_dbhh))\n",
    "        dWxh_error = abs(num_dWxh- analytic_dWxh) / (abs(num_dWxh) + abs(analytic_dWxh))\n",
    "        dbxh_error = abs(num_dbxh - analytic_dbxh) / (abs(num_dbxh) + abs(analytic_dbxh))\n",
    "        \n",
    "        # Softmax differences\n",
    "        dWs_error = abs(num_dWs - analytic_dWs) / (abs(num_dWs) + abs(analytic_dWs))\n",
    "        dbs_error = abs(num_dbs - analytic_dbs) / (abs(num_dbs) + abs(analytic_dbs))\n",
    "\n",
    "        try:\n",
    "            assert(np.linalg.norm(dWhh_error) < 1e-6 and np.linalg.norm(dbhh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWxh_error) < 1e-6 and np.linalg.norm(dbxh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWs_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "        except AssertionError:\n",
    "            warn('Gradient check failed!')\n",
    "            \n",
    "            # Hidden and input differences\n",
    "            warn('dWhh relative error: {}'.format(dWhh_error))\n",
    "            warn('dbhh relative error: {}'.format(dbhh_error))\n",
    "            warn('dWxh relative error: {}'.format(dWxh_error))\n",
    "            warn('dbxh relative error: {}'.format(dbxh_error))\n",
    "            \n",
    "            # Softmax differences\n",
    "            warn('dWs relative error: {}'.format(dWs_error))\n",
    "            warn('dbs relative error: {}'.format(dbs_error))\n",
    "            \n",
    "    def numerical_gradients(self):\n",
    "        \"\"\"Compute numerical gradients of f with respect to self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, and self.bs\n",
    "\n",
    "        Returns approximation for df/dWhh, df/dbhh, df/dWhh, df/dbhh, df/dWs, df/dbs\n",
    "\n",
    "        \"\"\"\n",
    "        dWhh, dbhh = np.zeros_like(self.Whh), np.zeros_like(self.bhh)\n",
    "        dWxh, dbxh = np.zeros_like(self.Wxh), np.zeros_like(self.bxh)\n",
    "        dWs, dbs = np.zeros_like(self.Ws), np.zeros_like(self.bs)\n",
    "        \n",
    "        Whh, bhh, Wxh, bxh, Ws, bs = self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs\n",
    "        \n",
    "        step = 1e-5\n",
    "    \n",
    "        # df/dWhh\n",
    "        h = np.zeros_like(self.Whh)\n",
    "        it = np.nditer(Whh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWhh[ix] = (self.forward_backward_prop(Whh=Whh+h).loss - self.forward_backward_prop(Whh=Whh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bhh)\n",
    "        it = np.nditer(bhh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbhh[ix] = (self.forward_backward_prop(bhh=bhh+h).loss - self.forward_backward_prop(bhh=bhh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWxh\n",
    "        h = np.zeros_like(self.Wxh)\n",
    "        it = np.nditer(Wxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWxh[ix] = (self.forward_backward_prop(Wxh=Wxh+h).loss - self.forward_backward_prop(Wxh=Wxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bxh)\n",
    "        it = np.nditer(bxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbxh[ix] = (self.forward_backward_prop(bxh=bxh+h).loss - self.forward_backward_prop(bxh=bxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWs\n",
    "        h = np.zeros_like(self.Ws)\n",
    "        it = np.nditer(Ws, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWs[ix] = (self.forward_backward_prop(Ws=Ws+h).loss - self.forward_backward_prop(Ws=Ws-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbs\n",
    "        h = np.zeros_like(self.bs)\n",
    "        it = np.nditer(bs, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbs[ix] = (self.forward_backward_prop(bs=bs+h).loss - self.forward_backward_prop(bs=bs-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "\n",
    "        return dWhh, dbhh, dWxh, dbxh, dWs, dbs\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"Get a snapshot of the model's most recent activity\"\"\"\n",
    "        \n",
    "        return Snapshot(self.xs, self.ys,\n",
    "                        self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs,\n",
    "                        self.dWhh, self.dbhh, self.dWxh, self.dbxh, self.dWs, self.dbs,\n",
    "                        self.dhiddens, self.dhiddens_local, self.dhiddens_downstream,\n",
    "                        self.scores, self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.rnn import X_train, Y_train\n",
    "\n",
    "rnn = RecurrentNeuralNetwork(X_train, Y_train, H=3, C=3, learning_rate=.1, regularizer=0, gradient_checking=False, inspect=True)\n",
    "\n",
    "def states(iters):\n",
    "    for _ in range(iters):\n",
    "        rnn.learn()\n",
    "        yield rnn.info\n",
    "        \n",
    "states = list(states(5000))\n",
    "\n",
    "df = pd.DataFrame(states, columns=Snapshot._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebanner/.anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:117: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 4.05645764,  3.89256315, -4.0764823 , -2.45390417],\n",
       "        [-3.03874568, -3.52415002, -3.70710764,  3.6521689 ],\n",
       "        [-4.25793333, -4.04171786,  3.14218233, -2.74751789]]),\n",
       " array([0, 0, 2, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF4ZJREFUeJzt3XmMHOWdxvHv4wMbfGDMYcA2MRImwSwEggQOOWhyGidy\nLiIcCciiZCEIFARRFoGImOSP7IGiELIsWCIhHCvIKgTLSCYLKDQJEhgIYwewCRgMGAw2YPA1GGzm\nt39Ujadperp7Zrqreqaej1TqOt5+6/WLePqdt6u6FBGYmdnoNybvBpiZWTYc+GZmBeHANzMrCAe+\nmVlBOPDNzArCgW9mVhBNBb6ksZK6Jd1V41hJ0pb0eLekK1rfTDMzG65xTZa7CFgNTBng+AMRsag1\nTTIzs3ZoOMKXNAtYCNwAaKBirWyUmZm1XjNTOr8Efgz0DnA8gJMlrZK0XNK8lrXOzMxapm7gS/oq\nsCkiuhl4FP84MDsiPg78Glja2iaamVkrqN5v6Uj6OXAWsBuYCEwF7oiIs+u8Zx1wQkRsrtrvH+0x\nMxuCiGjJtHndEX5EXB4RsyPicGAx8OfqsJc0Q5LS9RNJPkQ216iOiPASwZVXXpl7GzplcV+4L9wX\n9ZdWavYqnT2ZDSDpvDTAlwCnA+dL2g30kHwwmJlZh2k68CPiAeCBdH1Jxf5rgWtb3zQzM2sl32mb\ng1KplHcTOob7op/7op/7oj3qfmnb0hNJkdW5zMxGC0lEFl/ampnZ6OHANzMrCAe+mVlBOPDNzArC\ngW9mVhAOfDOzgnDgm5kVhAPfzKwgHPhmZgXhwDczKwgHvplZQTjwzcwKwoFvZlYQDnwzs4JoKvAl\njZXULemuAY5fI+lZSaskHd/aJpqZWSs0O8K/CFhN+ojDSpIWAkdExFzgXOC61jXPzMxapWHgS5oF\nLARuAGr9CP8i4CaAiFgBTJM0o5WNNDOz4WtmhP9L4MdA7wDHZwLrK7ZfBmYNs11mZtZidR9iLumr\nwKaI6JZUqle0arvmswyvvLILpSVLpZKfW2lmVqVcLlMul9tSd91n2kr6OXAWsBuYCEwF7oiIsyvK\nXA+UI+L2dPtp4JSI2FhVV+zcGUyY0Pp/hJnZaJXZM20j4vKImB0RhwOLgT9Xhn1qGXB22rD5wNvV\nYd/nnXda0GIzMxuSulM6NQSApPMAImJJRCyXtFDSWmAHcM5Ab965c8jtNDOzYao7pdPSE0mxbl0w\nZ04mpzMzGxUym9JpNU/pmJnlJ9PA95SOmVl+HPhmZgXhKR0zs4LwCN/MrCA8wjczKwgHvplZQWQa\n+D09WZ7NzMwqOfDNzArCgW9mVhAOfDOzgnDgm5kVhAPfzKwgHPhmZgXhwDczKwgHvplZQTQMfEkT\nJa2QtFLSk5K6apQpSdoiqTtdrqhVlwPfzCw/DR9xGBE7JZ0aET2SxgEPSro7IlZUFX0gIhbVq2vH\njuE01czMhqOpKZ2I6Bub7wWMB3prFGv4CC6P8M3M8tNU4EsaI2klsBG4JyIerSoSwMmSVklaLmle\nrXoc+GZm+Wk4pQMQEb3AcZL2Be6UdHREPFVR5HFgdjrtcxqwFDiyup5Nm7ro6krWS6USpVJpeK03\nMxtlyuUy5XK5LXUrIgb3BuknQE9E/KJOmXXACRGxuWJfTJ4cbNs25LaamRWOJCKi4ZR5M5q5SucA\nSdPS9b2BLwJrqsrMkKR0/USSD5LN1XX19MAgP1/MzKxFmpnSOQS4SdJYkg+I30fEcknnAUTEEuB0\n4HxJu4EeYHHNk42D996DCRNa03gzM2veoKd0hnwiKaZNC55/HvbbL5NTmpmNeJlO6bTSPvv4Sh0z\ns7w48M3MCsKBb2ZWEA58M7OCcOCbmRWEA9/MrCAc+GZmBeHANzMrCAe+mVlBOPDNzArCgW9mVhAO\nfDOzgnDgm5kVhAPfzKwgHPhmZgXhwDczK4i6gS9poqQVklZKelJS1wDlrpH0rKRVko4fqD4HvplZ\nfuoGfkTsBE6NiOOA44AFkk6qLCNpIXBERMwFzgWuG6g+B76ZWX4aTulERF9E7wWMB3qriiwCbkrL\nrgCmSZpRqy4HvplZfhoGvqQxklYCG4F7IuLRqiIzgfUV2y8Ds2rV5cA3M8vPuEYFIqIXOE7SvsCd\nko6OiKeqilU/YLfmk9Gvv76LV1+Fri4olUqUSqWhtNnMbNQql8uUy+W21K2Imtlcu7D0E6AnIn5R\nse96oBwRt6fbTwOnRMTGqvfGG28ERx4Jb77ZmsabmY12koiI6kH1kDS6SucASdPS9b2BLwJrqoot\nA85Oy8wH3q4O+z6e0jEzy0+jKZ1DgJskjSX5cPh9RCyXdB5ARCxJtxdKWgvsAM4ZqLKJE+Hdd6G3\nF8ZkegeAmZkNakpnWCeSIiKYNAk2bYJJkzI5rZnZiJbZlE47eFrHzCwfDnwzs4Jw4JuZFYQD38ys\nIBz4ZmYFkXngT5rkwDczy4NH+GZmBeHANzMriFwCf8eOrM9qZmaZB/7kybB9e9ZnNTOzzAN/yhTY\nti3rs5qZmUf4ZmYF4cA3MysIT+mYmRWER/hmZgXRzEPMZ0u6X9JTkp6U9MMaZUqStkjqTpcrBqrP\nI3wzs3w0fIg5sAu4OCJWSpoM/E3SvRFR/ajDByJiUaPKPMI3M8tHwxF+RLwWESvT9e0kz7Q9tEbR\npp7I4hG+mVk+BjWHL2kOcDywoupQACdLWiVpuaR5A9XhEb6ZWT6amdIBIJ3O+QNwUTrSr/Q4MDsi\neiSdBiwFjqxVj0f4Zmb5aCrwJY0H7gBujYil1ccjYlvF+t2S/lvS9IjYXFmuq6uL99+HLVvg/vtL\nnHpqaZjNNzMbXcrlMuVyuS11KyLqF5AE3AS8GREXD1BmBrApIkLSicD/RsScqjLRd64JE2Dr1uTV\nzMwGJomIaOo70kaaGeF/CjgT+Luk7nTf5cBhABGxBDgdOF/SbqAHWFyvwr5pHQe+mVl2Go7wW3ai\nihH+nDlQLievZmY2sFaO8DO/0xb8xa2ZWR5yCXxfmmlmlj2P8M3MCsIjfDOzgsgt8D3CNzPLVm5T\nOh7hm5lly1M6ZmYF4S9tzcwKwiN8M7OC8AjfzKwgPMI3MysIj/DNzArCI3wzs4LIbYS/dWseZzYz\nK65cAn/qVE/pmJllLbfA9wjfzCxbDQNf0mxJ90t6StKTkn44QLlrJD0raZWk4+vV6cA3M8teMyP8\nXcDFEXE0MB+4QNJRlQUkLQSOiIi5wLnAdfUq7Hu04bvvDqHFZmY2JA0DPyJei4iV6fp2YA1waFWx\nRSQPOiciVgDT0gebD2jffT3KNzPL0qDm8CXNAY4HVlQdmgmsr9h+GZhVr66pU2HLlsGc3czMhqPp\nwJc0GfgDcFE60v9Qkartuk9H9zy+mVm2xjVTSNJ44A7g1ohYWqPIK8Dsiu1Z6b4P6Orq2rPe21ti\n69bSIJpqZjb6lctlyuVyW+pWRN2BOJJEMj//ZkRcPECZhcCFEbFQ0nzg6oiYX1UmKs+1aBF8//vJ\nq5mZ1SaJiKieQRmSZkb4nwLOBP4uqTvddzlwGEBELImI5ZIWSloL7ADOaVSpp3TMzLLVMPAj4kGa\nu5rnwsGc2IFvZpatXO60BV+WaWaWtdwC35dlmpllK9fA9wjfzCw7Dnwzs4Jw4JuZFYQD38ysIBz4\nZmYF4cA3MyuIXK/D92WZZmbZ8QjfzKwgcgv8CROgt9dPvTIzy0pugS8lo/xt2/JqgZlZseQW+OBp\nHTOzLDnwzcwKwoFvZlYQuQa+L800M8tOw8CX9FtJGyU9McDxkqQtkrrT5YpmT+4RvplZdpp5xOGN\nwK+Bm+uUeSAiBv10Wge+mVl2mnl04V+BtxoUG9IDdh34ZmbZacUcfgAnS1olabmkec2+0YFvZpad\nZqZ0GnkcmB0RPZJOA5YCR9Yq2NXVtWe9VCoxdWqJ115rQQvMzEaJcrlMuVxuS92KiMaFpDnAXRFx\nTBNl1wEnRMTmqv1Rfa6bb4Z774VbbhlMk83MikMSETGkafNqw57SkTRDktL1E0k+RDY3eBsA++0H\nbzX6dsDMzFqi4ZSOpNuAU4ADJK0HrgTGA0TEEuB04HxJu4EeYHGzJ58+HTY39dFgZmbD1dSUTktO\nVGNKZ/Vq+Na3YM2aTJpgZjbidNSUznB4SsfMLDsdEfgZ/ZFhZlZouQb+xIkwZgz09OTZCjOzYsg1\n8MHTOmZmWXHgm5kVRO6BP326A9/MLAu5B75H+GZm2eiIwPfNV2Zm7dcRge8RvplZ+znwzcwKwoFv\nZlYQDnwzs4LIPfD9i5lmZtnIPfD33x/efDPvVpiZjX65B/5BB8GmTXm3wsxs9Ms98GfMSALfv5hp\nZtZeDQNf0m8lbZT0RJ0y10h6VtIqSccPpgGTJoEEO3YM5l1mZjZYzYzwbwQWDHRQ0kLgiIiYC5wL\nXDfYRnhax8ys/RoGfkT8Fah34eQi4Ka07ApgmqQZg2mEA9/MrP1aMYc/E1hfsf0yMGswFTjwzcza\nb1yL6ql+wG7Nr2C7urr2rJdKJUqlEuDANzPrUy6XKZfLbalb0cTlMZLmAHdFxDE1jl0PlCPi9nT7\naeCUiNhYVS4GOtdll8GUKXD55YNuv5nZqCaJiKgeVA9JK6Z0lgFnA0iaD7xdHfaNeIRvZtZ+Dad0\nJN0GnAIcIGk9cCUwHiAilkTEckkLJa0FdgDnDLYRBx0Ejzwy2HeZmdlgNAz8iPhOE2UuHE4jDj0U\nXn11ODWYmVkjud9pCzBrFqxf37icmZkNXVNf2rbkRHW+tH3nneRnknt6YExHfASZmXWGTvvSdtj2\n3ju5Suf11/NuiZnZ6NURgQ/JtM7LL+fdCjOz0atjAn/2bM/jm5m1kwPfzKwgOibwZ82Cl17KuxVm\nZqNXxwT+EUfAc8/l3Qozs9GrYwJ/7lx49tm8W2FmNnp1xHX4ANu3w4EHJk++8rX4ZmaJUXcdPsDk\nyTB9ui/NNDNrl44JfEimdZ55Ju9WmJmNTh0V+EceCf/4R96tMDMbnToq8I89FlatyrsVZmajU0cF\n/ic+Ad3debfCzGx06pirdCC5Uuegg2DLFhg/PpNmmZl1tMyv0pG0QNLTkp6VdGmN4yVJWyR1p8sV\nQ2nM5Mlw2GGwZs1Q3m1mZvU084jDscB/AV8AXgEelbQsIqpj+YGIWDTcBp1wAjz2WDKfb2ZmrdPM\nCP9EYG1EvBARu4Dbga/VKNeSPzk+8xn4y19aUZOZmVVqJvBnApW/Y/lyuq9SACdLWiVpuaR5Q21Q\nqQTl8lDfbWZmA2k4pUMS5o08DsyOiB5JpwFLgSOrC3V1de1ZL5VKlEqlD1X00Y/Cu+/CCy/AnDlN\nnNnMbBQpl8uU2zTqbXiVjqT5QFdELEi3LwN6I+I/6rxnHXBCRGyu2NfwKp0+Z54Jn/40/OAHTRU3\nMxu1sr5K5zFgrqQ5kvYCzgCWVTVohiSl6yeSfJBs/nBVzfnGN+CPfxzqu83MrJamrsNPp2muBsYC\nv4mIf5N0HkBELJF0AXA+sBvoAS6JiIer6mh6hL9jBxx6KKxbl/ygmplZUbVyhN9RN15V+va34fOf\n97SOmRXbqPx55GrnnQfXXQcZfR6ZmY16HRv4n/sc7NwJDz6Yd0vMzEaHjg38MWPgRz+Cn/4075aY\nmY0OHRv4AOecAy++CPfdl3dLzMxGvo4O/PHj4aqr4IIL4J138m6NmdnI1rFX6VRavBgOPhiuvrrF\njTIz63CFuEqn0rXXwl13we9+l3dLzMxGrmZ+Syd3+++fBH6pBNOmwde/nneLzMxGnhER+ADz5sHd\nd8NXvgJvvgnf+17eLTIzG1lGxBx+pTVr4JvfhPnz4Ve/gqlTW9A4M7MOVbg5/EpHHQWPPgrjxsHH\nPga33ALvv593q8zMOt+IG+FXeughuOQSeOstuPRSOOMM2Geflp7CzCxXhfjxtGZFJE/IuuoqePjh\nZLpn8WL47Gdhr71afjozs0w58AewYQPceivceSesXp38Hs+pp8InPwkf/7g/AMxs5HHgN+H11+Ge\ne5IHoj/0EDz/PBx7LBxzTHLFz9FHJ6+HHAJqSVeambVepoEvaQH9Dz+5odajDSVdA5xG8vCTf46I\n7hplMg38atu2wd/+loz8n3oqWVavhu3b4bDD4CMfSZY5c2DmTJgxAw46KHk98ED/dWBm+cgs8CWN\nBf4BfAF4BXgU+E5ErKkosxC4MCIWSjoJ+FVEzK9RV66BP5Dt2+Gll5KHpr/4YvK6YQNs3AibNiWv\nb7wBU6YkHwDTpyc3f02bBvvu279euT1pUv+yzz79rxMmJH9NlMvlmg9wLyL3RT/3RT/3Rb9WBn6j\nG69OBNZGxAvpiW8HvgasqSizCLgJICJWSJomaUZEbGxFA9tt8uRkamfevIHL9PbC5s1J+L/1FmzZ\nAm+/nSxbtiQ3gj33XP/2jh39S09P/+vu3Un49/aWmTGjtOfDYMKE5C+ICRMGXuodHzcu+aG5ceM+\nuDSzr1aZceOSn6fOgv/H7ue+6Oe+aI9GgT8TWF+x/TJwUhNlZgEjIvCbMWYMHHBAsgzHrl1J8P/s\nZ3D++f0fBu++C++9l7wOtPQd37r1w8d27/7gsmvX8PeNGZME/9ixyfqYMf3r1a/1jjUq88ILyXcs\nzZxDSpbK9ertkXoMYNWq5KID+OD+ytdG+/I+3qo6N2xIpmDbfc7KMq1ab0fdrdIo8Judg6luWufN\n3XSA8eOTaZ8pU+CII/JuzcAikr9qdu1KXnt7k5vbar3WO9ZMmRtvhDPPbK6eiP6l3narjr3/fjbn\n6+vz556DP/2p/3jf/r7XRvvyPt7KOjdsgMcea+85K8u0ar2d9bVCozn8+UBXRCxIty8Deiu/uJV0\nPVCOiNvT7aeBU6qndCT5Q8DMbAiymsN/DJgraQ6wATgD+E5VmWXAhcDt6QfE27Xm71vVYDMzG5q6\ngR8RuyVdCPwfyWWZv4mINZLOS48viYjlkhZKWgvsAM5pe6vNzGzQMrvxyszM8pXJxXeSFkh6WtKz\nki7N4pxZkvRbSRslPVGxb7qkeyU9I+keSdMqjl2W9sXTkr5Usf8ESU+kx36V9b+jFSTNlnS/pKck\nPSnph+n+wvWHpImSVkhamfZFV7q/cH3RR9JYSd2S7kq3C9kXkl6Q9Pe0Lx5J97W/LyKirQvJVNBa\nYA4wHlgJHNXu82a5AJ8BjgeeqNj3n8C/puuXAv+ers9L+2B82idr6f9L6xHgxHR9ObAg73/bEPri\nYOC4dH0yyY17RxW4P/ZJX8cBD5Nc1lzIvkjbfgnwP8CydLuQfQGsA6ZX7Wt7X2Qxwt9z81ZE7AL6\nbt4aNSLir8BbVbv33JCWvvY9mPFrwG0RsSuSG9rWAidJOgSYEhGPpOVurnjPiBERr0XEynR9O8lN\nejMpbn/0pKt7kfwPGxS0LyTNAhYCN9B/KXch+yJVfSFL2/sii8CvdWPWzAzOm7fKu403AjPS9UNJ\n+qBPX39U73+FEd5P6dVdxwMrKGh/SBojaSXJv/me9H/OQvYF8Evgx0Bvxb6i9kUA90l6TNK/pPva\n3hdZPNO28N8KR0QU7T4ESZOBO4CLImKbKm4bLFJ/REQvcJykfYE7Jf1T1fFC9IWkrwKbIqJbUqlW\nmaL0RepTEfGqpAOBe9P7l/ZoV19kMcJ/BZhdsT2bD34qjVYbJR0MkP7ptSndX90fs0j645V0vXL/\nKxm0s+UkjScJ+1siYmm6u7D9ARARW4D7gS9TzL44GVgkaR1wG/A5SbdQzL4gIl5NX18H7iSZ+m57\nX2QR+Htu3pK0F8nNW8syOG/elgHfTde/Cyyt2L9Y0l6SDgfmAo9ExGvAVkknKRkOn1XxnhEjbftv\ngNURcXXFocL1h6QD+q60kLQ38EWS7zQK1xcRcXlEzI6Iw4HFwJ8j4iwK2BeS9pE0JV2fBHwJeIIs\n+iKjb6RPI7laYy1wWd7fkLfh33cbyZ3I75F8X3EOMB24D3gGuAeYVlH+8rQvnga+XLH/hPQ//Frg\nmrz/XUPsi0+TzNGuBLrTZUER+wM4BngcWJX+O65I9xeuL6r65RT6r9IpXF8Ah6f/f6wEnuzLxCz6\nwjdemZkVREa/em5mZnlz4JuZFYQD38ysIBz4ZmYF4cA3MysIB76ZWUE48M3MCsKBb2ZWEP8PXNq5\nv3feL+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e1c2df898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loss.plot()\n",
    "\n",
    "rnn.predict(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
