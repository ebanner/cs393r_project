{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolithic Recurrent Neural Network\n",
    "\n",
    "Vectorized 2-layer fully-connected recurrent neural network with cross-entropy loss. Includes support for:\n",
    "\n",
    "- Minibatching\n",
    "- Optional gradient checking\n",
    "- L2 Regularization\n",
    "- Logging so you can see what the scores, probabilities, gradients, etc. are after every minibatch\n",
    "\n",
    "This is a shallow network with hard-coded sigmoid units. Additionally this architecture assumes each sequence is the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple('State', ['loss', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs'])\n",
    "Snapshot = namedtuple('State', ['xs', 'ys', 'Whh', 'bhh', 'Wxh', 'bxh', 'Ws', 'bs', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs', 'dhiddens', 'dhiddens_local', 'dhiddens_downstream', 'scores', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.shallow.helper import sigmoid, sigmoid_grad\n",
    "from softmax import softmax_vectorized\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    If you want to inspect the scores after each training example, the pass inspect. If you do\n",
    "    this then you better set a batch_size to 1. Otherwise you'll only ever get the scores of\n",
    "    the last training example in the minibatch\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X, ys_train, H, C,\n",
    "                 Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None,\n",
    "                 rollout=None, learning_rate=0.001, regularizer=1.,\n",
    "                gradient_checking=False, inspect=False):\n",
    "        \"\"\"Initializes recurrent neural network classifier\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : N x M 2d array containing all elements in the sequence\n",
    "        ys_train : length M list of labels\n",
    "        H : size of hidden layer\n",
    "        C : number of target classes\n",
    "        Whh : H x H 2d matrix mapping previous hidden layer to new hidden layer\n",
    "        bhh : H x 1 array of bias terms applied after Whh multiplication\n",
    "        Wxh : H x N 2d matrix mapping input at time t to hidden size\n",
    "        bxh : H x 1 array of bias terms applied after Wxh multiplication\n",
    "        Ws : C x H matrix of softmax weights\n",
    "        bs : C x 1 array of softmax biases\n",
    "        rollout : the number of tokens to train the rnn on in one go\n",
    "        learning_rate : learning rate constant\n",
    "        regularizer : regularization constant\n",
    "        gradient_checking : boolean whether to perform gradient checking during training\n",
    "        inspect : boolean whether to log all data after every learning session from a training example\n",
    "        \n",
    "        \"\"\"\n",
    "        (self.N, self.M) = X.shape\n",
    "        self.H = H\n",
    "        \n",
    "        self.X_train, self.ys_train = X, ys_train\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = np.random.randn(H, H) if not type(Whh) == np.ndarray else Whh\n",
    "        self.bhh = np.random.randn(H, 1) if not type(bhh) == np.ndarray else bhh\n",
    "        self.Wxh = np.random.randn(H, self.N) if not type(Wxh) == np.ndarray else Wxh\n",
    "        self.bxh = np.random.randn(H, 1) if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = np.random.randn(C, H) if not type(Ws) == np.ndarray else Ws\n",
    "        self.bs = np.random.randn(C, 1) if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        self.rollout = self.M if not rollout else rollout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularizer = regularizer\n",
    "        \n",
    "        self.gradient_checking = gradient_checking\n",
    "        self.inspect = inspect\n",
    "        \n",
    "        self.train_index = 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the probability of x belonging to either class\"\"\"\n",
    "        \n",
    "        # Create artificial labels just to make forward_backward_prop() happy\n",
    "        N, T = X.shape\n",
    "        ys = np.ones(T, dtype=np.int)\n",
    "        \n",
    "        scores = self.forward_backward_prop(X, ys, rollout=T, predict=True)\n",
    "        proper_scores = np.hstack([score for t, score in sorted(scores.items())])\n",
    "        \n",
    "        return proper_scores, proper_scores.argmax(axis=0)\n",
    "        \n",
    "    def forward_backward_prop(self, X=None, ys=None, rollout=None, train_index=None, Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None, predict=False):\n",
    "        \"\"\"Perform forward and backward prop over a single training example\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        # Hidden and input weights\n",
    "        Whh = self.Whh if not type(Whh) == np.ndarray else Whh\n",
    "        bhh = self.bhh if not type(bhh) == np.ndarray else bhh\n",
    "        Wxh = self.Wxh if not type(Wxh) == np.ndarray else Wxh\n",
    "        bxh = self.bxh if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        Ws = self.Ws if not type(Ws) == np.ndarray else Ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        # rollout and train_index will be set to T and 0 respectively if we are predicting\n",
    "        rollout = self.rollout if not rollout else rollout\n",
    "        train_index = self.train_index if not train_index else train_index\n",
    "        \n",
    "        # Passed in X to predict?\n",
    "        X = self.X_train[:, train_index:train_index+rollout] if not type(X) == np.ndarray else X\n",
    "        ys = self.ys_train[train_index:train_index+rollout] if not type(ys) == np.ndarray else ys\n",
    "        \n",
    "        # Append column of zeros to align X and Y with natural time\n",
    "        N, T = X.shape\n",
    "        X, ys = np.hstack([np.zeros((N, 1)), X]), np.hstack([np.zeros(1, dtype=np.int), ys])\n",
    "        \n",
    "        # Forward pass!\n",
    "        dWhh, dbhh = np.zeros_like(Whh), np.zeros_like(bhh)\n",
    "        dWxh, dbxh = np.zeros_like(Wxh), np.zeros_like(bxh)\n",
    "        dWs, dbs = np.zeros_like(Ws), np.zeros_like(bs)\n",
    "        \n",
    "        loss = 0.\n",
    "        hiddens, dhiddens = {t:np.ones((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        dhiddens_downstream, dhiddens_local = {t:np.zeros((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        scores, probs = {t:None for t in range(1, rollout+1)}, {t:None for t in range(1, rollout+1)}\n",
    "        for t in range(1, T+1):\n",
    "            # Previous hidden layer and input at time t\n",
    "            Z = (Whh @ hiddens[t-1] + bhh) + (Wxh @ X[:,[t]] + bxh)\n",
    "            hiddens[t] = sigmoid(Z)\n",
    "            \n",
    "            # Softmax\n",
    "            scores[t] = Ws @ hiddens[t] + bs\n",
    "            probs[t] = softmax_vectorized(scores[t])\n",
    "            y_hat = probs[t][ys[t]]\n",
    "\n",
    "            # Loss\n",
    "            loss += -np.log(y_hat).sum()\n",
    "\n",
    "        # Add regularization\n",
    "        loss += self.regularizer * 0.5*(np.sum(Whh**2) + np.sum(bhh**2) +\n",
    "                                        np.sum(Wxh**2) + np.sum(bxh**2) +\n",
    "                                        np.sum(Ws**2) + np.sum(bs**2))\n",
    "        if predict:\n",
    "            return scores\n",
    "        \n",
    "        # Backpropagate!\n",
    "        backwards = list(reversed(range(T+1)))\n",
    "        for t in backwards[:-1]:\n",
    "            # Scores\n",
    "            dscores = probs[t]\n",
    "            dscores[ys[t], 0] -= 1\n",
    "\n",
    "            # Softmax weights\n",
    "            dbs += dscores\n",
    "            dWs += dscores @ hiddens[t].T\n",
    "\n",
    "            # Karpathy optimization\n",
    "            dhiddens_local[t] = Ws.T @ dscores\n",
    "            dhiddens[t] = dhiddens_local[t] + dhiddens_downstream[t]\n",
    "            \n",
    "            dZ = sigmoid_grad(hiddens[t]) * dhiddens[t]\n",
    "\n",
    "            # Input and hidden weights\n",
    "            dbxh += dZ\n",
    "            dWxh += dZ @ X[:,[t]].T\n",
    "            dbhh += dZ\n",
    "            dWhh += dZ @ hiddens[t-1].T\n",
    "            \n",
    "            # Set up incoming hidden weight gradient for previous time step\n",
    "            dhiddens_downstream[t-1] = Whh.T @ dZ\n",
    "        \n",
    "        # Regularization\n",
    "        #\n",
    "        # Hidden and input weights\n",
    "        dWhh += (self.regularizer*Whh)\n",
    "        dbhh += (self.regularizer*bhh)\n",
    "        dWxh += (self.regularizer*Wxh)\n",
    "        dbxh += (self.regularizer*bxh)\n",
    "        \n",
    "        # Softmax weights\n",
    "        dWs += (self.regularizer*Ws)\n",
    "        dbs += (self.regularizer*bs)\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            self.xs, self.ys = str(X[:, 1:]), str(ys[1:])\n",
    "            self.scores, self.probs = scores, probs\n",
    "            self.loss = loss\n",
    "            self.dWhh, self.dbhh, self.dWxh, self.dbxh = dWhh, dbhh, dWxh, dbxh\n",
    "            self.dWs, self.dbs = dWs, dbs\n",
    "            self.dhiddens = dhiddens\n",
    "            self.dhiddens_local, self.dhiddens_downstream = dhiddens_local, dhiddens_downstream\n",
    "        \n",
    "        return State(loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Learn from a minibatch of training examples\n",
    "        \n",
    "        Run gradient descent on these examples\n",
    "        \n",
    "        \"\"\"\n",
    "        loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs = self.forward_backward_prop()\n",
    "\n",
    "        self.gradient_check(dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = self.Whh - self.learning_rate*dWhh\n",
    "        self.bhh = self.bhh - self.learning_rate*dbhh\n",
    "        self.Wxh = self.Wxh - self.learning_rate*dWxh\n",
    "        self.bxh = self.bxh - self.learning_rate*dbxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = self.Ws - self.learning_rate*dWs\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.train_index = (self.train_index+self.rollout) % self.M\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            pass\n",
    "    \n",
    "    def gradient_check(self, analytic_dWhh, analytic_dbhh, analytic_dWxh, analytic_dbxh, analytic_dWs, analytic_dbs):\n",
    "        \"\"\"Verify gradient correctness\n",
    "        \n",
    "        The analytic dWhh, dbhh, dWxh, dbxh, dWs, and dbs come from doing forward-backward\n",
    "        prop just a second ago. We numerically estimate these gradients on\n",
    "        the *same* minibatch the analytic gradients were computed from and\n",
    "        compare them to see if they are close.\n",
    "        \n",
    "        Note the same rollout is being used because this function gets\n",
    "        called *before* the update to batch_index\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.gradient_checking:\n",
    "            return\n",
    "        \n",
    "        num_dWhh, num_dbhh, num_dWxh, num_dbxh, num_dWs, num_dbs = self.numerical_gradients()\n",
    "\n",
    "        # Compute relative error\n",
    "        #\n",
    "        # Hidden and input differences\n",
    "        dWhh_error = abs(num_dWhh- analytic_dWhh) / (abs(num_dWhh) + abs(analytic_dWhh))\n",
    "        dbhh_error = abs(num_dbhh - analytic_dbhh) / (abs(num_dbhh) + abs(analytic_dbhh))\n",
    "        dWxh_error = abs(num_dWxh- analytic_dWxh) / (abs(num_dWxh) + abs(analytic_dWxh))\n",
    "        dbxh_error = abs(num_dbxh - analytic_dbxh) / (abs(num_dbxh) + abs(analytic_dbxh))\n",
    "        \n",
    "        # Softmax differences\n",
    "        dWs_error = abs(num_dWs - analytic_dWs) / (abs(num_dWs) + abs(analytic_dWs))\n",
    "        dbs_error = abs(num_dbs - analytic_dbs) / (abs(num_dbs) + abs(analytic_dbs))\n",
    "\n",
    "        try:\n",
    "            assert(np.linalg.norm(dWhh_error) < 1e-6 and np.linalg.norm(dbhh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWxh_error) < 1e-6 and np.linalg.norm(dbxh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWs_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "        except AssertionError:\n",
    "            warn('Gradient check failed!')\n",
    "            \n",
    "            # Hidden and input differences\n",
    "            warn('dWhh relative error: {}'.format(dWhh_error))\n",
    "            warn('dbhh relative error: {}'.format(dbhh_error))\n",
    "            warn('dWxh relative error: {}'.format(dWxh_error))\n",
    "            warn('dbxh relative error: {}'.format(dbxh_error))\n",
    "            \n",
    "            # Softmax differences\n",
    "            warn('dWs relative error: {}'.format(dWs_error))\n",
    "            warn('dbs relative error: {}'.format(dbs_error))\n",
    "            \n",
    "    def numerical_gradients(self):\n",
    "        \"\"\"Compute numerical gradients of f with respect to self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, and self.bs\n",
    "\n",
    "        Returns approximation for df/dWhh, df/dbhh, df/dWhh, df/dbhh, df/dWs, df/dbs\n",
    "\n",
    "        \"\"\"\n",
    "        dWhh, dbhh = np.zeros_like(self.Whh), np.zeros_like(self.bhh)\n",
    "        dWxh, dbxh = np.zeros_like(self.Wxh), np.zeros_like(self.bxh)\n",
    "        dWs, dbs = np.zeros_like(self.Ws), np.zeros_like(self.bs)\n",
    "        \n",
    "        Whh, bhh, Wxh, bxh, Ws, bs = self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs\n",
    "        \n",
    "        step = 1e-5\n",
    "    \n",
    "        # df/dWhh\n",
    "        h = np.zeros_like(self.Whh)\n",
    "        it = np.nditer(Whh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWhh[ix] = (self.forward_backward_prop(Whh=Whh+h).loss - self.forward_backward_prop(Whh=Whh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bhh)\n",
    "        it = np.nditer(bhh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbhh[ix] = (self.forward_backward_prop(bhh=bhh+h).loss - self.forward_backward_prop(bhh=bhh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWxh\n",
    "        h = np.zeros_like(self.Wxh)\n",
    "        it = np.nditer(Wxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWxh[ix] = (self.forward_backward_prop(Wxh=Wxh+h).loss - self.forward_backward_prop(Wxh=Wxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bxh)\n",
    "        it = np.nditer(bxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbxh[ix] = (self.forward_backward_prop(bxh=bxh+h).loss - self.forward_backward_prop(bxh=bxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWs\n",
    "        h = np.zeros_like(self.Ws)\n",
    "        it = np.nditer(Ws, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWs[ix] = (self.forward_backward_prop(Ws=Ws+h).loss - self.forward_backward_prop(Ws=Ws-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbs\n",
    "        h = np.zeros_like(self.bs)\n",
    "        it = np.nditer(bs, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbs[ix] = (self.forward_backward_prop(bs=bs+h).loss - self.forward_backward_prop(bs=bs-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "\n",
    "        return dWhh, dbhh, dWxh, dbxh, dWs, dbs\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"Get a snapshot of the model's most recent activity\"\"\"\n",
    "        \n",
    "        return Snapshot(self.xs, self.ys,\n",
    "                        self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs,\n",
    "                        self.dWhh, self.dbhh, self.dWxh, self.dbxh, self.dWs, self.dbs,\n",
    "                        self.dhiddens, self.dhiddens_local, self.dhiddens_downstream,\n",
    "                        self.scores, self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.rnn import X_train, Y_train\n",
    "\n",
    "rnn = RecurrentNeuralNetwork(X_train, Y_train, H=3, C=3, learning_rate=.1, regularizer=0, gradient_checking=False, inspect=True)\n",
    "\n",
    "def states(iters):\n",
    "    for _ in range(iters):\n",
    "        rnn.learn()\n",
    "        yield rnn.info\n",
    "        \n",
    "states = list(states(5000))\n",
    "\n",
    "df = pd.DataFrame(states, columns=Snapshot._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0818ff7b8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd088f72470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Scores as RNN Trains Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1 -1  1]]\n",
      "[0 0 2 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEACAYAAAAZcwXkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JAhK0kN/R+vRvP2m0+DOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd08175a2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "N, T = X_train.shape\n",
    "\n",
    "@interact(X_train=fixed(X_train), Y_train=fixed(Y_train), iter_index=(0, len(states)-1))\n",
    "def plot(X_train, Y_train, iter_index):\n",
    "    s = states[iter_index]\n",
    "    \n",
    "    rnn = RecurrentNeuralNetwork(X=X_train, ys_train=Y_train, H=3, C=3,\n",
    "                                 Whh=s.Whh, bhh=s.bhh, Wxh=s.Wxh, bxh=s.bxh, Ws=s.Ws, bs=s.bs)\n",
    "    \n",
    "    scores, predictions = rnn.predict(X_train)\n",
    "    \n",
    "    print(X_train)\n",
    "    print(Y_train)\n",
    "    \n",
    "    plt.imshow(scores, interpolation='nearest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
