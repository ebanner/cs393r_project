{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolithic Recurrent Neural Network\n",
    "\n",
    "Vectorized 2-layer fully-connected recurrent neural network with cross-entropy loss. Includes support for:\n",
    "\n",
    "- Minibatching\n",
    "- Optional gradient checking\n",
    "- L2 Regularization\n",
    "- Logging so you can see what the scores, probabilities, gradients, etc. are after every minibatch\n",
    "\n",
    "This is a shallow network with hard-coded sigmoid units. Additionally this architecture assumes each sequence is the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple('State', ['loss', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs'])\n",
    "Snapshot = namedtuple('State', ['xs', 'ys', 'Whh', 'bhh', 'Wxh', 'bxh', 'Ws', 'bs', 'dWhh', 'dbhh', 'dWxh', 'dbxh', 'dWs', 'dbs', 'dhiddens', 'dhiddens_local', 'dhiddens_downstream', 'scores', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.shallow.helper import sigmoid, sigmoid_grad\n",
    "from softmax import softmax_vectorized\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    If you want to inspect the scores after each training example, the pass inspect. If you do\n",
    "    this then you better set a batch_size to 1. Otherwise you'll only ever get the scores of\n",
    "    the last training example in the minibatch\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X, ys_train, H, C,\n",
    "                 Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None,\n",
    "                 rollout=None, learning_rate=0.001, regularizer=1.,\n",
    "                gradient_checking=False, inspect=False):\n",
    "        \"\"\"Initializes recurrent neural network classifier\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : N x M 2d array containing all elements in the sequence\n",
    "        ys_train : length M list of labels\n",
    "        H : size of hidden layer\n",
    "        C : number of target classes\n",
    "        Whh : H x H 2d matrix mapping previous hidden layer to new hidden layer\n",
    "        bhh : H x 1 array of bias terms applied after Whh multiplication\n",
    "        Wxh : H x N 2d matrix mapping input at time t to hidden size\n",
    "        bxh : H x 1 array of bias terms applied after Wxh multiplication\n",
    "        Ws : C x H matrix of softmax weights\n",
    "        bs : C x 1 array of softmax biases\n",
    "        rollout : the number of tokens to train the rnn on in one go\n",
    "        learning_rate : learning rate constant\n",
    "        regularizer : regularization constant\n",
    "        gradient_checking : boolean whether to perform gradient checking during training\n",
    "        inspect : boolean whether to log all data after every learning session from a training example\n",
    "        \n",
    "        \"\"\"\n",
    "        (self.N, self.M) = X.shape\n",
    "        self.H = H\n",
    "        \n",
    "        self.X_train, self.ys_train = X, ys_train\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = np.random.randn(H, H) if not type(Whh) == np.ndarray else Whh\n",
    "        self.bhh = np.random.randn(H, 1) if not type(bhh) == np.ndarray else bhh\n",
    "        self.Wxh = np.random.randn(H, self.N) if not type(Wxh) == np.ndarray else Wxh\n",
    "        self.bxh = np.random.randn(H, 1) if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = np.random.randn(C, H) if not type(Ws) == np.ndarray else Ws\n",
    "        self.bs = np.random.randn(C, 1) if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        self.rollout = self.M if not rollout else rollout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularizer = regularizer\n",
    "        \n",
    "        self.train_index = 0\n",
    "        \n",
    "        self.gradient_checking = gradient_checking\n",
    "        self.inspect = inspect\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the probability of x belonging to either class\"\"\"\n",
    "        \n",
    "        scores = self.forward_backward_prop(X, predict=True)\n",
    "        proper_scores = np.hstack([score for t, score in sorted(scores.items())])\n",
    "        \n",
    "        return proper_scores, proper_scores.argmax(axis=0)\n",
    "        \n",
    "    def forward_backward_prop(self, X=None, Whh=None, bhh=None, Wxh=None, bxh=None, Ws=None, bs=None, predict=False):\n",
    "        \"\"\"Perform forward and backward prop over a single training example\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        # Hidden and input weights\n",
    "        Whh = self.Whh if not type(Whh) == np.ndarray else Whh\n",
    "        bhh = self.bhh if not type(bhh) == np.ndarray else bhh\n",
    "        Wxh = self.Wxh if not type(Wxh) == np.ndarray else Wxh\n",
    "        bxh = self.bxh if not type(bxh) == np.ndarray else bxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        Ws = self.Ws if not type(Ws) == np.ndarray else Ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        # Passed in X to predict?\n",
    "        if type(X) == np.ndarray:\n",
    "            N, T = X.shape\n",
    "            rollout = T\n",
    "            ys = np.ones(T)\n",
    "        else:\n",
    "            X = self.X_train[:, self.train_index:self.train_index+self.rollout]\n",
    "            ys = self.ys_train[self.train_index:self.train_index+self.rollout]\n",
    "            rollout = self.rollout\n",
    "        \n",
    "        # Append column of zeros to make X and Y aligned in time\n",
    "        X, ys = np.hstack([np.zeros((self.N, 1)), X]), np.hstack([np.zeros(1, dtype=np.int), ys])\n",
    "        \n",
    "        # Forward pass!\n",
    "        dWhh, dbhh = np.zeros_like(Whh), np.zeros_like(bhh)\n",
    "        dWxh, dbxh = np.zeros_like(Wxh), np.zeros_like(bxh)\n",
    "        dWs, dbs = np.zeros_like(Ws), np.zeros_like(bs)\n",
    "        \n",
    "        loss = 0.\n",
    "        hiddens, dhiddens = {t:np.ones((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        dhiddens_downstream, dhiddens_local = {t:np.zeros((self.H, 1)) for t in range(rollout+1)}, {t:np.zeros((self.H, 1)) for t in range(rollout+1)}\n",
    "        scores, probs = {t:None for t in range(1, rollout+1)}, {t:None for t in range(1, rollout+1)}\n",
    "        for t in range(1, rollout+1):\n",
    "            # Previous hidden layer and input at time t\n",
    "            Z = (Whh @ hiddens[t-1] + bhh) + (Wxh @ X[:,[t]] + bxh)\n",
    "            hiddens[t] = sigmoid(Z)\n",
    "            \n",
    "            # Softmax\n",
    "            scores[t] = Ws @ hiddens[t] + bs\n",
    "            probs[t] = softmax_vectorized(scores[t])\n",
    "            y_hat = probs[t][ys[t]]\n",
    "\n",
    "            # Loss\n",
    "            loss += -np.log(y_hat).sum()\n",
    "\n",
    "        # Add regularization\n",
    "        loss += self.regularizer * 0.5*(np.sum(Whh**2) + np.sum(bhh**2) +\n",
    "                                        np.sum(Wxh**2) + np.sum(bxh**2) +\n",
    "                                        np.sum(Ws**2) + np.sum(bs**2))\n",
    "        if predict:\n",
    "            return scores\n",
    "        \n",
    "        # Backpropagate!\n",
    "        backwards = list(reversed(range(rollout+1)))\n",
    "        for t in backwards[:-1]:\n",
    "            # Scores\n",
    "            dscores = probs[t]\n",
    "            dscores[ys[t], 0] -= 1\n",
    "\n",
    "            # Softmax weights\n",
    "            dbs += dscores\n",
    "            dWs += dscores @ hiddens[t].T\n",
    "\n",
    "            # Karpathy optimization\n",
    "            dhiddens_local[t] = Ws.T @ dscores\n",
    "            dhiddens[t] = dhiddens_local[t] + dhiddens_downstream[t]\n",
    "            \n",
    "            dZ = sigmoid_grad(hiddens[t]) * dhiddens[t]\n",
    "\n",
    "            # Input and hidden weights\n",
    "            dbxh += dZ\n",
    "            dWxh += dZ @ X[:,[t]].T\n",
    "            dbhh += dZ\n",
    "            dWhh += dZ @ hiddens[t-1].T\n",
    "            \n",
    "            # Set up incoming hidden weight gradient for previous time step\n",
    "            dhiddens_downstream[t-1] = Whh.T @ dZ\n",
    "        \n",
    "        # Regularization\n",
    "        #\n",
    "        # Hidden and input weights\n",
    "        dWhh += (self.regularizer*Whh)\n",
    "        dbhh += (self.regularizer*bhh)\n",
    "        dWxh += (self.regularizer*Wxh)\n",
    "        dbxh += (self.regularizer*bxh)\n",
    "        \n",
    "        # Softmax weights\n",
    "        dWs += (self.regularizer*Ws)\n",
    "        dbs += (self.regularizer*bs)\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            self.xs, self.ys = str(X[:, 1:]), str(ys[1:])\n",
    "            self.scores, self.probs = scores, probs\n",
    "            self.loss = loss\n",
    "            self.dWhh, self.dbhh, self.dWxh, self.dbxh = dWhh, dbhh, dWxh, dbxh\n",
    "            self.dWs, self.dbs = dWs, dbs\n",
    "            self.dhiddens = dhiddens\n",
    "            self.dhiddens_local, self.dhiddens_downstream = dhiddens_local, dhiddens_downstream\n",
    "        \n",
    "        return State(loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Learn from a minibatch of training examples\n",
    "        \n",
    "        Run gradient descent on these examples\n",
    "        \n",
    "        \"\"\"\n",
    "        loss, dWhh, dbhh, dWxh, dbxh, dWs, dbs = self.forward_backward_prop()\n",
    "\n",
    "        self.gradient_check(dWhh, dbhh, dWxh, dbxh, dWs, dbs)\n",
    "        \n",
    "        # Hidden and input weights\n",
    "        self.Whh = self.Whh - self.learning_rate*dWhh\n",
    "        self.bhh = self.bhh - self.learning_rate*dbhh\n",
    "        self.Wxh = self.Wxh - self.learning_rate*dWxh\n",
    "        self.bxh = self.bxh - self.learning_rate*dbxh\n",
    "        \n",
    "        # Softmax weights\n",
    "        self.Ws = self.Ws - self.learning_rate*dWs\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.train_index = (self.train_index+self.rollout) % self.M\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            pass\n",
    "    \n",
    "    def gradient_check(self, analytic_dWhh, analytic_dbhh, analytic_dWxh, analytic_dbxh, analytic_dWs, analytic_dbs):\n",
    "        \"\"\"Verify gradient correctness\n",
    "        \n",
    "        The analytic dWhh, dbhh, dWxh, dbxh, dWs, and dbs come from doing forward-backward\n",
    "        prop just a second ago. We numerically estimate these gradients on\n",
    "        the *same* minibatch the analytic gradients were computed from and\n",
    "        compare them to see if they are close.\n",
    "        \n",
    "        Note the same rollout is being used because this function gets\n",
    "        called *before* the update to batch_index\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.gradient_checking:\n",
    "            return\n",
    "        \n",
    "        num_dWhh, num_dbhh, num_dWxh, num_dbxh, num_dWs, num_dbs = self.numerical_gradients()\n",
    "\n",
    "        # Compute relative error\n",
    "        #\n",
    "        # Hidden and input differences\n",
    "        dWhh_error = abs(num_dWhh- analytic_dWhh) / (abs(num_dWhh) + abs(analytic_dWhh))\n",
    "        dbhh_error = abs(num_dbhh - analytic_dbhh) / (abs(num_dbhh) + abs(analytic_dbhh))\n",
    "        dWxh_error = abs(num_dWxh- analytic_dWxh) / (abs(num_dWxh) + abs(analytic_dWxh))\n",
    "        dbxh_error = abs(num_dbxh - analytic_dbxh) / (abs(num_dbxh) + abs(analytic_dbxh))\n",
    "        \n",
    "        # Softmax differences\n",
    "        dWs_error = abs(num_dWs - analytic_dWs) / (abs(num_dWs) + abs(analytic_dWs))\n",
    "        dbs_error = abs(num_dbs - analytic_dbs) / (abs(num_dbs) + abs(analytic_dbs))\n",
    "\n",
    "        try:\n",
    "            assert(np.linalg.norm(dWhh_error) < 1e-6 and np.linalg.norm(dbhh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWxh_error) < 1e-6 and np.linalg.norm(dbxh_error) < 1e-6 and\n",
    "                   np.linalg.norm(dWs_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "        except AssertionError:\n",
    "            warn('Gradient check failed!')\n",
    "            \n",
    "            # Hidden and input differences\n",
    "            warn('dWhh relative error: {}'.format(dWhh_error))\n",
    "            warn('dbhh relative error: {}'.format(dbhh_error))\n",
    "            warn('dWxh relative error: {}'.format(dWxh_error))\n",
    "            warn('dbxh relative error: {}'.format(dbxh_error))\n",
    "            \n",
    "            # Softmax differences\n",
    "            warn('dWs relative error: {}'.format(dWs_error))\n",
    "            warn('dbs relative error: {}'.format(dbs_error))\n",
    "            \n",
    "    def numerical_gradients(self):\n",
    "        \"\"\"Compute numerical gradients of f with respect to self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, and self.bs\n",
    "\n",
    "        Returns approximation for df/dWhh, df/dbhh, df/dWhh, df/dbhh, df/dWs, df/dbs\n",
    "\n",
    "        \"\"\"\n",
    "        dWhh, dbhh = np.zeros_like(self.Whh), np.zeros_like(self.bhh)\n",
    "        dWxh, dbxh = np.zeros_like(self.Wxh), np.zeros_like(self.bxh)\n",
    "        dWs, dbs = np.zeros_like(self.Ws), np.zeros_like(self.bs)\n",
    "        \n",
    "        Whh, bhh, Wxh, bxh, Ws, bs = self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs\n",
    "        \n",
    "        step = 1e-5\n",
    "    \n",
    "        # df/dWhh\n",
    "        h = np.zeros_like(self.Whh)\n",
    "        it = np.nditer(Whh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWhh[ix] = (self.forward_backward_prop(Whh=Whh+h).loss - self.forward_backward_prop(Whh=Whh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bhh)\n",
    "        it = np.nditer(bhh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbhh[ix] = (self.forward_backward_prop(bhh=bhh+h).loss - self.forward_backward_prop(bhh=bhh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWxh\n",
    "        h = np.zeros_like(self.Wxh)\n",
    "        it = np.nditer(Wxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWxh[ix] = (self.forward_backward_prop(Wxh=Wxh+h).loss - self.forward_backward_prop(Wxh=Wxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbhh\n",
    "        h = np.zeros_like(self.bxh)\n",
    "        it = np.nditer(bxh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbxh[ix] = (self.forward_backward_prop(bxh=bxh+h).loss - self.forward_backward_prop(bxh=bxh-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWs\n",
    "        h = np.zeros_like(self.Ws)\n",
    "        it = np.nditer(Ws, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWs[ix] = (self.forward_backward_prop(Ws=Ws+h).loss - self.forward_backward_prop(Ws=Ws-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbs\n",
    "        h = np.zeros_like(self.bs)\n",
    "        it = np.nditer(bs, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbs[ix] = (self.forward_backward_prop(bs=bs+h).loss - self.forward_backward_prop(bs=bs-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "\n",
    "        return dWhh, dbhh, dWxh, dbxh, dWs, dbs\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"Get a snapshot of the model's most recent activity\"\"\"\n",
    "        \n",
    "        return Snapshot(self.xs, self.ys,\n",
    "                        self.Whh, self.bhh, self.Wxh, self.bxh, self.Ws, self.bs,\n",
    "                        self.dWhh, self.dbhh, self.dWxh, self.dbxh, self.dWs, self.dbs,\n",
    "                        self.dhiddens, self.dhiddens_local, self.dhiddens_downstream,\n",
    "                        self.scores, self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.rnn import X_train, Y_train\n",
    "\n",
    "rnn = RecurrentNeuralNetwork(X_train, Y_train, H=3, C=3, learning_rate=.1, regularizer=0, gradient_checking=False, inspect=True)\n",
    "\n",
    "def states(iters):\n",
    "    for _ in range(iters):\n",
    "        rnn.learn()\n",
    "        yield rnn.info\n",
    "        \n",
    "states = list(states(5000))\n",
    "\n",
    "df = pd.DataFrame(states, columns=Snapshot._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebanner/.anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:117: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 5.6104899 ,  5.42866034, -2.0179178 , -1.61509996],\n",
       "        [-1.47765254, -1.55582162, -1.40590214,  5.17605664],\n",
       "        [-3.42819083, -3.13253923,  5.50679096, -1.94011796]]),\n",
       " array([0, 0, 2, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nckkqnEEuSYX7f5Kne0POwt05AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e1c3196a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Scores as RNN Trains Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1 -1  1]]\n",
      "[0 0 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebanner/.anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:117: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEACAYAAAAZcwXkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nspLUkCErSQ0ZspLUkCErSQ39H9om/avQmrcQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e1ca41f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "N, T = X_train.shape\n",
    "\n",
    "@interact(X_train=fixed(X_train), Y_train=fixed(Y_train), iter_index=(0, len(states)-1))\n",
    "def plot(X_train, Y_train, iter_index):\n",
    "    s = states[iter_index]\n",
    "    \n",
    "    rnn = RecurrentNeuralNetwork(X=X_train, ys_train=Y_train, H=3, C=3,\n",
    "                                 Whh=s.Whh, bhh=s.bhh, Wxh=s.Wxh, bxh=s.bxh, Ws=s.Ws, bs=s.bs)\n",
    "    \n",
    "    scores, predictions = rnn.predict(X_train)\n",
    "    \n",
    "    print(X_train)\n",
    "    print(Y_train)\n",
    "    \n",
    "    plt.imshow(scores, interpolation='nearest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
