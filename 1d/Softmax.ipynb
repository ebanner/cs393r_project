{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Simple 1D softmax classifier model. Includes support for:\n",
    "\n",
    "- Training on minibatches of training examples at a time\n",
    "- Finite difference gradient checking\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Model = namedtuple('Model', ['ws', 'bs', 'dws', 'dbs', 'loss'])\n",
    "State = namedtuple('State', ['loss', 'dws', 'dbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"Compute the softmax between two numbers\n",
    "    \n",
    "    s1 is the number we're finding the softmax of\n",
    "    \n",
    "    \"\"\"\n",
    "    e_x = np.exp(scores)\n",
    "    \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, xs_train=1, ys_train=1, ws=None, bs=None, learning_rate=0.001, batch_size=None):\n",
    "        self.m = len(xs_train)\n",
    "        \n",
    "        self.xs_train, self.ys_train = xs_train, ys_train\n",
    "        self.ws, self.bs = np.array([50.,65.]), np.array([100.,150.])\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.batch_size = self.m if not batch_size else batch_size\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def forward_backward_prop(self, ws=None, bs=None):\n",
    "        \"\"\"Perform forward and backward prop over a minibatch of training examples\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        ws = self.ws if not type(ws) == np.ndarray else ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        loss, dws, dbs = 0., np.array([0.,0.]), np.array([0.,0.])\n",
    "        lower, upper = self.batch_index*self.batch_size, (self.batch_index+1)*self.batch_size\n",
    "        for x, y in zip(self.xs_train[lower:upper], self.ys_train[lower:upper]):\n",
    "            # Forward propagation\n",
    "            scores = np.array([x,x])*ws + bs\n",
    "            probs = softmax(scores)\n",
    "\n",
    "            # Accumulate loss\n",
    "            loss += -np.log(probs[y])\n",
    "\n",
    "            # Backpropagate to accumulate dbs and dws\n",
    "            dloss = 1\n",
    "            dscores = probs\n",
    "            dscores[y] -= 1\n",
    "            dbs += dscores\n",
    "            dws += x * dscores\n",
    "        \n",
    "        return State(loss/self.m, dws/self.m, dbs/self.m)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Run one iteration of gradient descent with a minibatch of training examples\"\"\"\n",
    "        \n",
    "        loss, dws, dbs = self.forward_backward_prop()\n",
    "        \n",
    "        self.ws = self.ws - self.learning_rate*dws\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.batch_index = (self.batch_index+1) % (self.m//self.batch_size)\n",
    "                \n",
    "        return Model(self.ws, self.bs, dws, dbs, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.softmax import xs_train, ys_train\n",
    "\n",
    "sm = Softmax(xs_train, ys_train, learning_rate=0.01, batch_size=2)\n",
    "\n",
    "def estimates(iters):\n",
    "    for _ in range(iters):\n",
    "        yield sm.step()\n",
    "        \n",
    "estimates = list(estimates(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does the Model Evolve Over Time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhiSpmaEhSWpmaEiSmhkakqRm/wYXKoxOCZYLtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27c34798d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "@interact(xs_train=fixed(xs_train), ys_train=fixed(ys_train), view_index=(0, len(estimates)), estimates=fixed(estimates))\n",
    "def plot(xs_train, ys_train, view_index, estimates):\n",
    "    ws, bs, _, _, _ = estimates[view_index]\n",
    "    \n",
    "    # Get data into suitable form for plotting\n",
    "    positives, negatives = xs_train[:len(xs_train)//2], xs_train[len(xs_train)//2:]\n",
    "    df1 = pd.DataFrame({'positives': positives, 'negatives': negatives, 'zeros': np.zeros_like(positives)})\n",
    "    \n",
    "    # Solve for the decision boundary\n",
    "    decision = (bs[1]-bs[0]) / (ws[0]-ws[1])\n",
    "    \n",
    "    ys = np.linspace(-.001,.001)\n",
    "    df2 = pd.DataFrame({'x': [decision], 'y': [0]})\n",
    "    \n",
    "    axes = df2.plot(kind='scatter', x='x', y='y', color='g', marker='|', s=10000)\n",
    "    axes = df1.plot(ax=axes, kind='scatter', x='positives', y='zeros', color='r', marker='+')\n",
    "    df1.plot(ax=axes, kind='scatter', x='negatives', y='zeros', color='b', marker='+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradients(f, ws, bs):\n",
    "    \"\"\"Compute numerical gradients of f with respect to w and b\n",
    "    \n",
    "    Returns approximation for df/dw and df/db\n",
    "    \n",
    "    \"\"\"\n",
    "    dws, dbs = np.zeros_like(ws), np.zeros_like(bs)\n",
    "    h = np.zeros_like(ws)\n",
    "    step = 0.00001\n",
    "    \n",
    "    # df/dw1 and df/db1\n",
    "    h[0] = step\n",
    "    dws[0] = (f(ws+h, bs)  -f(ws-h, bs))   / (2*step)\n",
    "    dbs[0] = (f(ws,   bs+h)-f(ws,   bs-h)) / (2*step)\n",
    "    h[0] = 0\n",
    "    \n",
    "    # df/dw1 and df/db1\n",
    "    h[1] = step\n",
    "    dws[1] = (f(ws+h, bs)  -f(ws-h, bs))   / (2*step)\n",
    "    dbs[1] = (f(ws,   bs+h)-f(ws,   bs-h)) / (2*step)\n",
    "    h[1] = 0\n",
    "    \n",
    "    return dws, dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Through a Few Iterations and Assert Analytic and Numerical Gradients are Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "sm = Softmax(xs_train, ys_train)\n",
    "\n",
    "for _ in range(10):\n",
    "    # Analytic gradient computed via backprop\n",
    "    _, analytic_dws, analytic_dbs = sm.forward_backward_prop()\n",
    "    \n",
    "    # Numerical gradient compute via twiddling w and b and inspecting loss\n",
    "    numerical_dws, numerical_dbs = numerical_gradients(lambda ws, bs: sm.forward_backward_prop(ws, bs).loss, sm.ws, sm.bs)\n",
    "    \n",
    "    # Compute relative error\n",
    "    dws_error = abs(numerical_dws - analytic_dws) / (abs(numerical_dws) + abs(analytic_dws))\n",
    "    dbs_error = abs(numerical_dbs - analytic_dbs) / (abs(numerical_dbs) + abs(analytic_dbs))\n",
    "    \n",
    "    try:\n",
    "        assert(np.linalg.norm(dws_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "    except AssertionError:\n",
    "        warn(dws_error)\n",
    "        warn(dbs_error)\n",
    "        \n",
    "else:\n",
    "    print('Gradient check passed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
