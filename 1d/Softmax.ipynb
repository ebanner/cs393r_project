{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Simple 1D softmax classifier model with cross-entropy loss. Includes support for:\n",
    "\n",
    "- Training on minibatches of training examples at a time\n",
    "- Finite difference gradient checking\n",
    "- L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Model = namedtuple('Model', ['ws', 'bs', 'dws', 'dbs', 'loss'])\n",
    "State = namedtuple('State', ['loss', 'dws', 'dbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"Compute the softmax between two numbers\n",
    "    \n",
    "    s1 is the number we're finding the softmax of\n",
    "    \n",
    "    \"\"\"\n",
    "    e_x = np.exp(scores)\n",
    "    \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, xs_train=1, ys_train=1, ws=None, bs=None, learning_rate=0.001, batch_size=None):\n",
    "        self.m = len(xs_train)\n",
    "        \n",
    "        self.xs_train, self.ys_train = xs_train, ys_train\n",
    "        self.ws, self.bs = np.array([50.,65.]), np.array([100.,150.])\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.batch_size = self.m if not batch_size else batch_size\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def forward_backward_prop(self, ws=None, bs=None):\n",
    "        \"\"\"Perform forward and backward prop over a minibatch of training examples\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        ws = self.ws if not type(ws) == np.ndarray else ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        loss, dws, dbs = 0., np.array([0.,0.]), np.array([0.,0.])\n",
    "        lower, upper = self.batch_index*self.batch_size, (self.batch_index+1)*self.batch_size\n",
    "        for x, y in zip(self.xs_train[lower:upper], self.ys_train[lower:upper]):\n",
    "            # Forward propagation\n",
    "            scores = np.array([x,x])*ws + bs\n",
    "            probs = softmax(scores)\n",
    "\n",
    "            # Accumulate performance loss and regularization loss\n",
    "            loss += -np.log(probs[y])\n",
    "            loss += 0.5*(ws**2).sum() + 0.5*(bs**2).sum()\n",
    "\n",
    "            # Backpropagate to accumulate dbs and dws\n",
    "            dloss = 1\n",
    "            dscores = probs\n",
    "            dscores[y] -= 1\n",
    "            dbs += dscores\n",
    "            dws += x * dscores\n",
    "            \n",
    "            # Add regularization factor\n",
    "            dws += ws\n",
    "            dbs += bs\n",
    "        \n",
    "        return State(loss/self.m, dws/self.m, dbs/self.m)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Run one iteration of gradient descent with a minibatch of training examples\"\"\"\n",
    "        \n",
    "        loss, dws, dbs = self.forward_backward_prop()\n",
    "        \n",
    "        self.ws = self.ws - self.learning_rate*dws\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.batch_index = (self.batch_index+1) % (self.m//self.batch_size)\n",
    "                \n",
    "        return Model(self.ws, self.bs, dws, dbs, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.softmax import xs_train, ys_train\n",
    "\n",
    "sm = Softmax(xs_train, ys_train, learning_rate=0.01, batch_size=2)\n",
    "\n",
    "def estimates(iters):\n",
    "    for _ in range(iters):\n",
    "        yield sm.step()\n",
    "        \n",
    "estimates = list(estimates(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does the Model Evolve Over Time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZEBIkpoMCElS0/8DUCpIMA9Ot1kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6bc0dcde10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "@interact(xs_train=fixed(xs_train), ys_train=fixed(ys_train), view_index=(0, len(estimates)), estimates=fixed(estimates))\n",
    "def plot(xs_train, ys_train, view_index, estimates):\n",
    "    ws, bs, _, _, _ = estimates[view_index]\n",
    "    \n",
    "    # Get data into suitable form for plotting\n",
    "    positives, negatives = xs_train[:len(xs_train)//2], xs_train[len(xs_train)//2:]\n",
    "    df1 = pd.DataFrame({'positives': positives, 'negatives': negatives, 'zeros': np.zeros_like(positives)})\n",
    "    \n",
    "    # Solve for the decision boundary\n",
    "    decision = (bs[1]-bs[0]) / (ws[0]-ws[1])\n",
    "    \n",
    "    ys = np.linspace(-.001,.001)\n",
    "    df2 = pd.DataFrame({'x': [decision], 'y': [0]})\n",
    "    \n",
    "    axes = df2.plot(kind='scatter', x='x', y='y', color='g', marker='|', s=10000)\n",
    "    axes = df1.plot(ax=axes, kind='scatter', x='positives', y='zeros', color='r', marker='+')\n",
    "    df1.plot(ax=axes, kind='scatter', x='negatives', y='zeros', color='b', marker='+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradients(f, ws, bs):\n",
    "    \"\"\"Compute numerical gradients of f with respect to w and b\n",
    "    \n",
    "    Returns approximation for df/dw and df/db\n",
    "    \n",
    "    \"\"\"\n",
    "    dws, dbs = np.zeros_like(ws), np.zeros_like(bs)\n",
    "    h = np.zeros_like(ws)\n",
    "    step = 0.00001\n",
    "    \n",
    "    # df/dw1 and df/db1\n",
    "    h[0] = step\n",
    "    dws[0] = (f(ws+h, bs)  -f(ws-h, bs))   / (2*step)\n",
    "    dbs[0] = (f(ws,   bs+h)-f(ws,   bs-h)) / (2*step)\n",
    "    h[0] = 0\n",
    "    \n",
    "    # df/dw1 and df/db1\n",
    "    h[1] = step\n",
    "    dws[1] = (f(ws+h, bs)  -f(ws-h, bs))   / (2*step)\n",
    "    dbs[1] = (f(ws,   bs+h)-f(ws,   bs-h)) / (2*step)\n",
    "    h[1] = 0\n",
    "    \n",
    "    return dws, dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Through a Few Iterations and Assert Analytic and Numerical Gradients are Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "sm = Softmax(xs_train, ys_train)\n",
    "\n",
    "for _ in range(10):\n",
    "    # Analytic gradient computed via backprop\n",
    "    _, analytic_dws, analytic_dbs = sm.forward_backward_prop()\n",
    "    \n",
    "    # Numerical gradient compute via twiddling w and b and inspecting loss\n",
    "    numerical_dws, numerical_dbs = numerical_gradients(lambda ws, bs: sm.forward_backward_prop(ws, bs).loss, sm.ws, sm.bs)\n",
    "    \n",
    "    # Compute relative error\n",
    "    dws_error = abs(numerical_dws - analytic_dws) / (abs(numerical_dws) + abs(analytic_dws))\n",
    "    dbs_error = abs(numerical_dbs - analytic_dbs) / (abs(numerical_dbs) + abs(analytic_dbs))\n",
    "    \n",
    "    try:\n",
    "        assert(np.linalg.norm(dws_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "    except AssertionError:\n",
    "        warn(dws_error)\n",
    "        warn(dbs_error)\n",
    "        \n",
    "else:\n",
    "    print('Gradient check passed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
