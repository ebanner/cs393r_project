{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolithic Neural Network\n",
    "\n",
    "Vectorized 2-layer fully-connected neural network with cross-entropy loss. Includes support for:\n",
    "\n",
    "- Minibatching\n",
    "- Optional gradient checking\n",
    "- L2 Regularization\n",
    "- Logging so you can see what the scores, probabilities, gradients, etc. are after every minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Model = namedtuple('Model', ['X', 'ys', 'Wh', 'bh', 'Z', 'hidden', 'Ws', 'bs', 'scores', 'probs', 'dscores', 'dWs', 'dbs', 'dhidden', 'dZ', 'dbh', 'dWh', 'loss'])\n",
    "State = namedtuple('State', ['loss', 'dWh', 'dbh', 'dWs', 'dbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.neural_net import sigmoid, sigmoid_grad\n",
    "from lib.softmax import softmax_vectorized\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Initialize model parameters\n",
    "    \n",
    "    Additionally calculate batch index so we can use minibatches with each training iteration\n",
    "    \n",
    "    If you want to inspect the scores after each training example, the pass inspect. If you do\n",
    "    this then you better set a batch_size to 1. Otherwise you'll only ever get the scores of\n",
    "    the last training example in the minibatch\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X, ys_train, H, C, W=None, b=None,\n",
    "                 learning_rate=0.001, regularizer=1., batch_size=1,\n",
    "                gradient_checking=False, inspect=False):\n",
    "        \"\"\"Initializes softmax classifier\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : N x M 2d array containing training input examples\n",
    "        ys_train : length M list of labels\n",
    "        H : size of hidden layer\n",
    "        C : number of target classes\n",
    "        W : C x M 2d array of class weights\n",
    "        b : C length list of biases\n",
    "        learning_rate : learning rate constant\n",
    "        regularizer : regularization constant\n",
    "        batch_size : size of minibatch\n",
    "        gradient_checking : boolean whether to perform gradient checking during training\n",
    "        inspect : boolean whether to log all data after every learning session from a training example\n",
    "        \n",
    "        \"\"\"\n",
    "        (self.N, self.M) = X.shape\n",
    "        \n",
    "        self.X_train, self.ys_train = X, ys_train\n",
    "        \n",
    "        self.Wh = np.random.randn(H, self.N)\n",
    "        self.bh = np.random.randn(H).reshape(H, 1)\n",
    "        self.Ws = np.random.randn(C, H)\n",
    "        self.bs = np.random.randn(C).reshape(C, 1)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularizer = regularizer\n",
    "        \n",
    "        self.batch_size = self.m if not batch_size else batch_size\n",
    "        self.batch_index = 0\n",
    "        \n",
    "        self.gradient_checking = gradient_checking\n",
    "        self.inspect = inspect\n",
    "        \n",
    "        # Info from the *last* minibatch that was used to learn from\n",
    "        self.X, self.ys = None, None\n",
    "        self.Z, self.hidden = None, None\n",
    "        self.scores, self.dscores = None, None\n",
    "        self.probs = None\n",
    "        self.dWs, self.dbs = None, None\n",
    "        self.dWh, self.dbh = None, None\n",
    "        self.loss = None\n",
    "        \n",
    "    def forward_backward_prop(self, Wh=None, bh=None, Ws=None, bs=None):\n",
    "        \"\"\"Perform forward and backward prop over a minibatch of training examples\n",
    "        \n",
    "        Returns loss and gradients\n",
    "        \n",
    "        \"\"\"\n",
    "        Wh = self.Wh if not type(Wh) == np.ndarray else Wh\n",
    "        bh = self.bh if not type(bh) == np.ndarray else bh\n",
    "        Ws = self.Ws if not type(Ws) == np.ndarray else Ws\n",
    "        bs = self.bs if not type(bs) == np.ndarray else bs\n",
    "        \n",
    "        # Get minibatch of training examples\n",
    "        low, high = self.batch_index*self.batch_size, (self.batch_index+1)*self.batch_size\n",
    "        X = self.X_train[:, low:high].reshape(self.N, self.batch_size)\n",
    "        ys = self.ys_train[low:high]\n",
    "        \n",
    "        # Forward Pass (predictions)\n",
    "        Z = Wh @ X + bh\n",
    "        hidden = sigmoid(Z)\n",
    "        scores = Ws @ hidden + bs\n",
    "        probs = softmax_vectorized(scores)\n",
    "        y_hats = probs[ys, range(self.batch_size)]\n",
    "\n",
    "        # Loss\n",
    "        losses = -np.log(y_hats)\n",
    "        loss = sum(losses)\n",
    "\n",
    "        # Backpropagate!\n",
    "        dscores = probs\n",
    "        dscores[ys, range(self.batch_size)] -= 1\n",
    "        \n",
    "        dbs = dscores.sum(axis=1, keepdims=True)\n",
    "        dWs = dscores @ hidden.T\n",
    "        \n",
    "        dhidden = Ws.T @ dscores\n",
    "        dZ = sigmoid_grad(hidden) * dhidden\n",
    "        \n",
    "        dbh = dZ.sum(axis=1, keepdims=True)\n",
    "        dWh = dZ @ X.T\n",
    "        \n",
    "        # Regularization\n",
    "        loss += self.regularizer * 0.5*((Wh**2).sum() + (bh**2).sum() + (Ws**2).sum() + (bs**2).sum())\n",
    "        \n",
    "        dWh += (self.regularizer*Wh)\n",
    "        dbh += (self.regularizer*bh)\n",
    "        dWs += (self.regularizer*Ws)\n",
    "        dbs += (self.regularizer*bs)\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            self.X, self.ys = X, ys\n",
    "            self.Z = Z\n",
    "            self.scores, self.dscores = scores, dscores\n",
    "            self.dscores[ys, range(self.batch_size)] += 1\n",
    "            self.probs = probs\n",
    "            self.dhidden = dhidden\n",
    "            self.dZ = dZ\n",
    "            self.bh = bh\n",
    "            self.Wh = Wh\n",
    "        \n",
    "        return State(loss/self.M, dWh/self.M, dbh/self.M, dWs/self.M, dbs/self.M)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Learn from a minibatch of training examples\n",
    "        \n",
    "        Run gradient descent on these examples\n",
    "        \n",
    "        \"\"\"        \n",
    "        loss, dWh, dbh, dWs, dbs = self.forward_backward_prop()\n",
    "\n",
    "        self.gradient_check(dWh, dbh, dWs, dbs)\n",
    "        \n",
    "        self.Wh = self.Wh - self.learning_rate*dWh\n",
    "        self.bh = self.bh - self.learning_rate*dbh\n",
    "        self.Ws = self.Ws - self.learning_rate*dWs\n",
    "        self.bs = self.bs - self.learning_rate*dbs\n",
    "        \n",
    "        # Update batch index so the next time the next batch in line is used\n",
    "        self.batch_index = (self.batch_index+1) % (self.M//self.batch_size)\n",
    "        \n",
    "        # Log additional info?\n",
    "        if self.inspect:\n",
    "            self.dWh = dWh\n",
    "            self.dbh = dbh\n",
    "            self.dWs = dWs\n",
    "            self.dbs = dbs\n",
    "            self.loss = loss\n",
    "    \n",
    "    def gradient_check(self, analytic_dWh, analytic_dbh, analytic_dWs, analytic_dbs):\n",
    "        \"\"\"Verify gradient correctness\n",
    "        \n",
    "        The analytic dWh, dbh, dWs, and dbs come from doing forward-backward\n",
    "        prop just a second ago. We numerically estimate these gradients on\n",
    "        the *same* minibatch the analytic gradients were computed from and\n",
    "        compare them to see if they are close.\n",
    "        \n",
    "        Note the same minibatch is being used because this function gets\n",
    "        called *before* the update to batch_index\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.gradient_checking:\n",
    "            return\n",
    "        \n",
    "        numerical_dWh, numerical_dbh, numerical_dWs, numerical_dbs = self.numerical_gradients()\n",
    "\n",
    "        # Compute relative error\n",
    "        dWh_error = abs(numerical_dWh - analytic_dWh) / (abs(numerical_dWh) + abs(analytic_dWh))\n",
    "        dbh_error = abs(numerical_dbh - analytic_dbh) / (abs(numerical_dbh) + abs(analytic_dbh))\n",
    "        dWs_error = abs(numerical_dWs - analytic_dWs) / (abs(numerical_dWs) + abs(analytic_dWs))\n",
    "        dbs_error = abs(numerical_dbs - analytic_dbs) / (abs(numerical_dbs) + abs(analytic_dbs))\n",
    "\n",
    "        try:\n",
    "            assert(np.linalg.norm(dWh_error) < 1e-6 and np.linalg.norm(dbh_error) < 1e-6\n",
    "                  and np.linalg.norm(dWs_error) < 1e-6 and np.linalg.norm(dbs_error) < 1e-6)\n",
    "        except AssertionError:\n",
    "            warn('Gradient check failed!')\n",
    "            warn('dWh relative error: {}'.format(dWh_error))\n",
    "            warn('dbh relative error: {}'.format(dbh_error))\n",
    "            warn('dWs relative error: {}'.format(dWs_error))\n",
    "            warn('dbs relative error: {}'.format(dbs_error))\n",
    "            \n",
    "    def numerical_gradients(self):\n",
    "        \"\"\"Compute numerical gradients of f with respect to self.Wh, self.bh, self.Ws, and self.bs\n",
    "\n",
    "        Returns approximation for df/dWh, df/dbh, df/dWs, df/dbs\n",
    "\n",
    "        \"\"\"\n",
    "        dWh, dbh, dWs, dbs = np.zeros_like(self.Wh), np.zeros_like(self.bh), np.zeros_like(self.Ws), np.zeros_like(self.bs)\n",
    "        Wh, bh, Ws, bs = self.Wh, self.bh, self.Ws, self.bs\n",
    "        \n",
    "        step = 1e-5\n",
    "    \n",
    "        # df/dWh\n",
    "        h = np.zeros_like(self.Wh)\n",
    "        it = np.nditer(Wh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWh[ix] = (self.forward_backward_prop(Wh+h, bh, Ws, bs).loss - self.forward_backward_prop(Wh-h, bh, Ws, bs).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbh\n",
    "        h = np.zeros_like(self.bh)\n",
    "        it = np.nditer(bh, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbh[ix] = (self.forward_backward_prop(Wh, bh+h, Ws, bs).loss - self.forward_backward_prop(Wh, bh-h, Ws, bs).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dWh\n",
    "        h = np.zeros_like(self.Ws)\n",
    "        it = np.nditer(Ws, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dWs[ix] = (self.forward_backward_prop(Wh, bh, Ws+h, bs).loss - self.forward_backward_prop(Wh, bh, Ws-h, bs).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "            \n",
    "        # df/dbs\n",
    "        h = np.zeros_like(self.bs)\n",
    "        it = np.nditer(bs, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            h[ix] = step\n",
    "            \n",
    "            dbs[ix] = (self.forward_backward_prop(Wh, bh, Ws, bs+h).loss - self.forward_backward_prop(Wh, bh, Ws, bs-h).loss) / (2*step)\n",
    "\n",
    "            h[ix] = 0\n",
    "            it.iternext()\n",
    "\n",
    "        return dWh, dbh, dWs, dbs\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"Get a snapshot of the model's most recent activity\"\"\"\n",
    "        \n",
    "        return Model(self.X, self.ys,\n",
    "                     self.Wh, self.bh, self.Z, self.hidden,\n",
    "                     self.Ws, self.bs,\n",
    "                     self.scores, self.probs, self.dscores,\n",
    "                     self.dbs, self.dWs,\n",
    "                     self.dhidden, self.dZ,\n",
    "                     self.dbh, self.dWh,\n",
    "                     self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.softmax import X_train, Y_train\n",
    "\n",
    "sm = NeuralNetwork(X_train, Y_train, H=1, C=2, learning_rate=.01, regularizer=0, batch_size=1, gradient_checking=True, inspect=True)\n",
    "\n",
    "def states(iters):\n",
    "    for _ in range(iters):\n",
    "        sm.learn()\n",
    "        yield sm.info\n",
    "        \n",
    "states = list(states(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(states, columns=Model._fields)\n",
    "\n",
    "Params = namedtuple('Params', ['x', 'y', 'label', 'w11', 'w21', 'w12', 'w22', 'b1', 'b2', 's1', 's2', 'p1', 'p2', 'ds1', 'ds2', 'db1', 'db2', 'dw11', 'dw21', 'dw12', 'dw22', 'loss'])\n",
    "\n",
    "def params():\n",
    "    for s in states:\n",
    "        yield Params(s.X[0,0], s.X[1,0], s.ys[0],\n",
    "                     s.W[0,0], s.W[1,0], s.W[0,1], s.W[1,1], s.b[0, 0], s.b[1, 0],\n",
    "                     s.scores[0, 0], s.scores[1, 0], s.probs[0, 0], s.probs[1, 0], s.dscores[0, 0], s.dscores[1, 0],\n",
    "                     s.db[0, 0], s.db[1, 0], s.dW[0,0], s.dW[1,0], s.dW[0,1], s.dW[1,1],\n",
    "                     s.loss)\n",
    "\n",
    "params = list(params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(params, columns=Params._fields)\n",
    "\n",
    "df.groupby(['x', 'y'])['loss'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['x', 'y'])[['s1', 's2']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "@interact(X_train=fixed(X_train), ys_train=fixed(ys_train), view_index=(0, len(states)), states=fixed(states))\n",
    "def plot(X_train, ys_train, view_index, states):\n",
    "    s = states[view_index]\n",
    "    X = X_train.T\n",
    "    \n",
    "    # Plot positives\n",
    "    poss = X[:len(X)//2]\n",
    "    xs, ys = poss[:, 0], poss[:, 1]\n",
    "    axes = pd.DataFrame({'x': xs, 'y': ys}).plot(kind='scatter', x='x', y='y', color='b', marker='+')\n",
    "    \n",
    "    # Plot negatives\n",
    "    negs = X[len(X)//2:]\n",
    "    xs, ys = negs[:, 0], negs[:, 1]\n",
    "    axes = pd.DataFrame({'x': xs, 'y': ys}).plot(ax=axes, kind='scatter', x='x', y='y', color='r', marker='+')\n",
    "    \n",
    "    # Generate decision boundary\n",
    "    xs = np.linspace(0, 3)\n",
    "    ys = (s.W[0,0]-s.W[1,0]*xs + (s.b[0]-s.b[1])) / (s.W[1,1]-s.W[0,1])\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    axes = pd.DataFrame(ys, index=xs).plot(ax=axes)\n",
    "    \n",
    "    axes.set_ylim(0, 3)\n",
    "    axes.set_xlim(0, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
